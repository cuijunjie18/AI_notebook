{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"生成y=Xw+b+噪声\"\"\"\n",
    "    X = torch.normal(0, 1, (num_examples, len(w)))\n",
    "    y = torch.matmul(X, w) + b\n",
    "    y += torch.normal(0, 0.01, y.shape)\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    # 这些样本是随机读取的，没有特定的顺序\n",
    "    random.shuffle(indices)\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(\n",
    "            indices[i: min(i + batch_size, num_examples)])\n",
    "        yield features[batch_indices], labels[batch_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X, w, b):  #@save\n",
    "    \"\"\"线性回归模型\"\"\"\n",
    "    return torch.matmul(X, w) + b\n",
    "\n",
    "def squared_loss(y_hat, y):  #@save\n",
    "    \"\"\"均方损失\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n",
    "\n",
    "def sgd(params, lr, batch_size):  #@save\n",
    "    \"\"\"小批量随机梯度下降\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 78.352867\n",
      "epoch 2, loss 356.175995\n",
      "epoch 3, loss 1623.370850\n",
      "epoch 4, loss 7417.300293\n",
      "epoch 5, loss 33969.054688\n",
      "epoch 6, loss 155908.093750\n",
      "epoch 7, loss 717038.125000\n",
      "epoch 8, loss 3304050.750000\n",
      "epoch 9, loss 15251993.000000\n",
      "epoch 10, loss 70522808.000000\n",
      "epoch 11, loss 326590912.000000\n",
      "epoch 12, loss 1514611072.000000\n",
      "epoch 13, loss 7033556480.000000\n",
      "epoch 14, loss 32702509056.000000\n",
      "epoch 15, loss 152222023680.000000\n",
      "epoch 16, loss 709292720128.000000\n",
      "epoch 17, loss 3308170248192.000000\n",
      "epoch 18, loss 15442928205824.000000\n",
      "epoch 19, loss 72147162628096.000000\n",
      "epoch 20, loss 337307739619328.000000\n",
      "epoch 21, loss 1578059434033152.000000\n",
      "epoch 22, loss 7387277713997824.000000\n",
      "epoch 23, loss 34600784817553408.000000\n",
      "epoch 24, loss 162145976180539392.000000\n",
      "epoch 25, loss 760195811094036480.000000\n",
      "epoch 26, loss 3565539462383403008.000000\n",
      "epoch 27, loss 16729696138239672320.000000\n",
      "epoch 28, loss 78523426096692592640.000000\n",
      "epoch 29, loss 368675818942500962304.000000\n",
      "epoch 30, loss 1731455179376256090112.000000\n"
     ]
    }
   ],
   "source": [
    "lr = 0.03\n",
    "num_epochs = 30\n",
    "batch_size = 10\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # for X, y in data_iter(batch_size, features, labels):\n",
    "    l = loss(net(features, w, b), labels)  # X和y的小批量损失\n",
    "    # 因为l形状是(batch_size,1)，而不是一个标量。l中的所有元素被加到一起，\n",
    "    # 并以此计算关于[w,b]的梯度\n",
    "    l.sum().backward()\n",
    "    sgd([w, b], lr, batch_size)  # 使用参数的梯度更新参数\n",
    "    with torch.no_grad():\n",
    "        train_l = loss(net(features, w, b), labels)\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的估计误差: tensor([ 3.8674e+10, -1.2667e+10], grad_fn=<SubBackward0>)\n",
      "b的估计误差: tensor([4.0298e+10], grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')\n",
    "print(f'b的估计误差: {true_b - b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
