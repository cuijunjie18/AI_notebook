{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **本节介绍torch中层和块的内部参数的相关操作**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入包\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**一，参数访问**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=4, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1611],\n",
       "        [-0.2229]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(2,4),nn.ReLU(),nn.Linear(4,1))\n",
    "print(net)\n",
    "x = torch.rand(2,2)\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'weight': tensor([[ 0.2748, -0.3534, -0.2412,  0.3502]]), 'bias': tensor([0.4890])})\n"
     ]
    }
   ],
   "source": [
    "print(net[2].state_dict()) # 访问Sequential中某一层的权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "<class 'torch.nn.parameter.Parameter'> \n",
      "\n",
      "Parameter containing:\n",
      "tensor([0.4890], requires_grad=True)\n",
      "tensor([0.4890])\n",
      "<class 'torch.Tensor'> \n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 单一权重参数访问\n",
    "print(type(net[2].bias))\n",
    "print(type(net[2].weight),\"\\n\")\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)\n",
    "print(type(net[2].bias.data),\"\\n\")\n",
    "print(net[2].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([4, 2])) ('bias', torch.Size([4])) \n",
      "\n",
      "('0.weight', torch.Size([4, 2])) ('0.bias', torch.Size([4])) ('2.weight', torch.Size([1, 4])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "# 递归访问全部权重\n",
    "print(*[(name,param.shape) for name,param in net[0].named_parameters()],\"\\n\")\n",
    "print(*[(name,param.shape) for name,param in net.named_parameters()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2748, -0.3534, -0.2412,  0.3502]])\n"
     ]
    }
   ],
   "source": [
    "# 单一权重的字典访问\n",
    "print(net.state_dict()['2.weight'].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**二，参数初始化**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵，\n",
    "这个范围是根据输入和输出维度计算出的。\n",
    "PyTorch的`nn.init`模块提供了多种预置初始化方法。\n",
    "\n",
    "下面我们试着调用内置的初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.7972, -0.2714],\n",
      "        [-0.6211, -0.1857],\n",
      "        [-0.8210,  0.6690],\n",
      "        [-0.7066, -0.4542]], requires_grad=True) Parameter containing:\n",
      "tensor([0., 0., 0., 0.], requires_grad=True) \n",
      " Parameter containing:\n",
      "tensor([[-1.7705, -0.4246,  0.0216, -2.1172]], requires_grad=True) Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 正态分布初始化\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight,mean = 0,std = 1) # 采用标准正态分布初始化权重矩阵\n",
    "        nn.init.zeros_(m.bias) # 偏置置零\n",
    "net.apply(init_normal)\n",
    "print(net[0].weight,net[0].bias,\"\\n\",net[2].weight,net[2].bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]], requires_grad=True) Parameter containing:\n",
      "tensor([0., 0., 0., 0.], requires_grad=True) \n",
      " Parameter containing:\n",
      "tensor([[1., 1., 1., 1.]], requires_grad=True) Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 常量初始化\n",
    "def init_constant(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight,1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_constant)\n",
    "print(net[0].weight,net[0].bias,\"\\n\",net[2].weight,net[2].bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.0790,  2.4374],\n",
      "        [ 0.7575,  1.1211],\n",
      "        [ 0.0847, -0.7577],\n",
      "        [-0.7633,  2.1732]], requires_grad=True) Parameter containing:\n",
      "tensor([0., 0., 0., 0.], requires_grad=True) \n",
      " Parameter containing:\n",
      "tensor([[1., 1., 1., 1.]], requires_grad=True) Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 对不同的块采用不同的初始化\n",
    "net[0].apply(init_normal)\n",
    "net[2].apply(init_constant)\n",
    "print(net[0].weight,net[0].bias,\"\\n\",net[2].weight,net[2].bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**下面进行自定义初始化**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有时，深度学习框架没有提供我们需要的初始化方法。\n",
    "在下面的例子中，我们使用以下的分布为任意权重参数$w$定义初始化方法：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    w \\sim \\begin{cases}\n",
    "        U(5, 10) & \\text{ 可能性 } \\frac{1}{4} \\\\\n",
    "            0    & \\text{ 可能性 } \\frac{1}{2} \\\\\n",
    "        U(-10, -5) & \\text{ 可能性 } \\frac{1}{4}\n",
    "    \\end{cases}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init weight torch.Size([4, 2])\n",
      "Init weight torch.Size([1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-6.8520, -8.9211],\n",
       "        [ 5.2097, -0.0000]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        print(\"Init\", *[(name, param.shape)\n",
    "                        for name, param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight, -10, 10)\n",
    "        m.weight.data *= m.weight.data.abs() >= 5\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[42.0000, -5.9211],\n",
      "        [ 8.2097,  3.0000],\n",
      "        [ 3.0000, 12.8282],\n",
      "        [11.7931,  8.2662]])\n",
      "tensor([42.0000, -5.9211])\n"
     ]
    }
   ],
   "source": [
    "# 也可以直接设置参数\n",
    "net[0].weight.data[:] += 1\n",
    "net[0].weight.data[0, 0] = 42\n",
    "print(net[0].weight.data)\n",
    "print(net[0].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**参数绑定**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "# 我们需要给共享层一个名称，以便可以引用它的参数\n",
    "shared = nn.Linear(8, 8)\n",
    "net = nn.Sequential(nn.Linear(2, 8), nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    shared, nn.ReLU(),\n",
    "                    nn.Linear(8, 1))\n",
    "net(x)\n",
    "# 检查参数是否相同\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])\n",
    "net[2].weight.data[0, 0] = 100\n",
    "# 确保它们实际上是同一个对象，而不只是有相同的值\n",
    "print(net[2].weight.data[0] == net[4].weight.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练一个具有共享参数的网络，观察梯度**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4958, -0.4090],\n",
      "        [-0.2610, -0.0809]]) tensor([ 0.9880, -0.3051])\n",
      "tensor([[ 0.4958, -0.4090],\n",
      "        [-0.2610, -0.0809]]) tensor([ 0.9880, -0.3051])\n",
      "tensor([[-0.4560,  0.0000]]) tensor([-1.6731])\n",
      "0.7211236953735352\n",
      "tensor([[ 0.4926, -0.4070],\n",
      "        [-0.2597, -0.0804]]) tensor([ 0.9834, -0.3036])\n",
      "tensor([[ 0.4926, -0.4070],\n",
      "        [-0.2597, -0.0804]]) tensor([ 0.9834, -0.3036])\n",
      "tensor([[-0.4518,  0.0000]]) tensor([-1.6679])\n",
      "0.7165688276290894\n",
      "tensor([[ 0.4895, -0.4051],\n",
      "        [-0.2584, -0.0800]]) tensor([ 0.9790, -0.3020])\n",
      "tensor([[ 0.4895, -0.4051],\n",
      "        [-0.2584, -0.0800]]) tensor([ 0.9790, -0.3020])\n",
      "tensor([[-0.4478,  0.0000]]) tensor([-1.6628])\n",
      "0.7120499610900879\n",
      "tensor([[ 0.4864, -0.4031],\n",
      "        [-0.2571, -0.0796]]) tensor([ 0.9745, -0.3005])\n",
      "tensor([[ 0.4864, -0.4031],\n",
      "        [-0.2571, -0.0796]]) tensor([ 0.9745, -0.3005])\n",
      "tensor([[-0.4437,  0.0000]]) tensor([-1.6577])\n",
      "0.7075669765472412\n",
      "tensor([[ 0.4834, -0.4011],\n",
      "        [-0.2558, -0.0792]]) tensor([ 0.9701, -0.2990])\n",
      "tensor([[ 0.4834, -0.4011],\n",
      "        [-0.2558, -0.0792]]) tensor([ 0.9701, -0.2990])\n",
      "tensor([[-0.4397,  0.0000]]) tensor([-1.6526])\n",
      "0.7031195163726807\n"
     ]
    }
   ],
   "source": [
    "shared = nn.Linear(2,2)\n",
    "model = nn.Sequential(shared,shared,nn.ReLU(),nn.Linear(2,1))\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# 生成简单数据集\n",
    "x = torch.rand(4,2)\n",
    "y = torch.rand(1)\n",
    "\n",
    "# 设置超参数\n",
    "lr = 1e-3\n",
    "wd = 0\n",
    "num_epoch = 5\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = lr,weight_decay = wd)\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred,y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for i in range(len(model)):\n",
    "        if (type(model[i]) == nn.Linear):\n",
    "            print(model[i].weight.grad,model[i].bias.grad)\n",
    "    optimizer.step()\n",
    "    print(loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_cv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
